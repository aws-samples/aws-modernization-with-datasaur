<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM Playgrounds on English</title>
    <link>//localhost:1313/5_modulefive_compare.html</link>
    <description>Recent content in LLM Playgrounds on English</description>
    <generator>Hugo</generator>
    <language>en</language>
    <atom:link href="//localhost:1313/5_modulefive_compare/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Getting Started with the LLM Playground</title>
      <link>//localhost:1313/5_modulefive_compare/41_gettingstarted.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/5_modulefive_compare/41_gettingstarted.html</guid>
      <description>Getting Started with the LLM Playground Step 1: Create Your LLM Playground Create LLM Playground&#xA;Click on &amp;ldquo;Create Playground&amp;rdquo; to establish your dedicated workspace. You can assign a descriptive name to your playground for easy identification. Step 2: Configure the Playground Application Configure the Application&#xA;Access the &amp;ldquo;Models Configuration&amp;rdquo; section within your playground. You can adjust various parameters for the connected LLM model, such as temperature or token settings. Experiment with different configurations to observe their impact on the model&amp;rsquo;s responses.</description>
    </item>
    <item>
      <title>Deploying the LLM</title>
      <link>//localhost:1313/5_modulefive_compare/42_deployingllm.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/5_modulefive_compare/42_deployingllm.html</guid>
      <description>Deploying the LLM Once you&amp;rsquo;ve crafted the perfect LLM configuration within the Playground, you can seamlessly transition it into a real-world application.&#xA;This configured Playground environment translates to your LLM Application. LLM Labs empowers you to effortlessly deploy your LLM Application, making it accessible via API for integration into your workflows.&#xA;Deployment Page&#xA;The deployment process is designed for simplicity. Here&amp;rsquo;s how to deploy your LLM Application:&#xA;Navigate to the Deployment Page: Within the LLM Playground, locate the dedicated deployment section.</description>
    </item>
    <item>
      <title>Cost Prediction Calculation</title>
      <link>//localhost:1313/5_modulefive_compare/43_costprediction.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/5_modulefive_compare/43_costprediction.html</guid>
      <description>Cost Prediction Calculation Overview The Cost Prediction for Inference feature aims to enhance user experience by providing transparency and predictability in terms of the costs incurred during language model inference. Users can now estimate the financial implications of their inferencing activities within LLM Lab.&#xA;How to see the cost prediction?&#xA;In the meantime, we only support cost prediction for OpenAI and Azure OpenAI models.&#xA;Open your LLM Application. Write your prompt query, and the predicted cost from your prompt templates will be shown.</description>
    </item>
  </channel>
</rss>
