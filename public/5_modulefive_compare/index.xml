<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sandbox on English</title>
    <link>//localhost:1313/5_modulefive_compare.html</link>
    <description>Recent content in Sandbox on English</description>
    <generator>Hugo</generator>
    <language>en</language>
    <atom:link href="//localhost:1313/5_modulefive_compare/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Getting Started with the Sandbox</title>
      <link>//localhost:1313/5_modulefive_compare/41_gettingstarted.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/5_modulefive_compare/41_gettingstarted.html</guid>
      <description>Getting Started with the Sandbox Step 1: Create Your Sandbox Create Sandbox&#xA;Click on &amp;ldquo;Create Sandbox&amp;rdquo; to establish your dedicated workspace. You can assign a descriptive name to your sandbox for easy identification. Step 2: Configure the Sandbox Application Configure the Application&#xA;Access the &amp;ldquo;Models Configuration&amp;rdquo; section. You can adjust various parameters for the connected LLM model, such as temperature or token settings. Here we will test multiple Amazon Bedrock models to discover the best performing and cost efficient model.</description>
    </item>
    <item>
      <title>Cost Prediction Calculation</title>
      <link>//localhost:1313/5_modulefive_compare/42_costprediction.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/5_modulefive_compare/42_costprediction.html</guid>
      <description>Cost Prediction Calculation Overview The Cost Prediction for Inference feature aims to enhance user experience by providing transparency and predictability in terms of the costs incurred during language model inference. Users can now estimate the financial implications of their inferencing activities within LLM Lab.&#xA;When looking at the responses that were returned from your prompt, youâ€™ll see a cost indication for each return. Cost Prediction Calculation&#xA;The cost prediction will calculate the cost based on your available prompt template, the more prompt template you have the cost will be more expensive.</description>
    </item>
    <item>
      <title>Deploying the LLM</title>
      <link>//localhost:1313/5_modulefive_compare/43_deployingllm.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/5_modulefive_compare/43_deployingllm.html</guid>
      <description>Deploying the LLM Once you&amp;rsquo;ve crafted the perfect LLM configuration within the Playground, you can seamlessly transition it into a real-world application.&#xA;This configured Playground environment translates to your LLM Application. LLM Labs empowers you to effortlessly deploy your LLM Application, making it accessible via API for integration into your workflows.&#xA;Deployment Page&#xA;The deployment process is designed for simplicity. Here&amp;rsquo;s how to deploy your LLM Application:&#xA;Navigate to the Deployment Page: Within the LLM Playground, locate the dedicated deployment section.</description>
    </item>
  </channel>
</rss>
