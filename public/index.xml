<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AWS - Datasaur Modernization Workshop on English</title>
    <link>//localhost:1313/</link>
    <description>Recent content in AWS - Datasaur Modernization Workshop on English</description>
    <generator>Hugo</generator>
    <language>en</language>
    <atom:link href="//localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>About Datasaur</title>
      <link>//localhost:1313/1_moduleone/11_aboutdatasaur.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/1_moduleone/11_aboutdatasaur.html</guid>
      <description>About Datasaur Mission At Datasaur, our mission is to transform the way organizations label data for NLP projects and configure LLMs specifically for their business. We aim to provide the most efficient, accurate, and secured data annotation solutions to fuel the NLP and LLM revolution.&#xA;Our Story Datasaur was founded by Ivan Lee, who recognized the growing need for high-quality data labeling in the AI and machine learning industries. With a background in technology and a passion for innovation, Ivan set out to create a platform that would address the challenges of data annotation.</description>
    </item>
    <item>
      <title>Explore the Models</title>
      <link>//localhost:1313/3_modulethree/31_explorethemodels.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/3_modulethree/31_explorethemodels.html</guid>
      <description>Explore the Models Navigate to the Models page under LLM Labs menu. Here you can see a lot of models that you can use.</description>
    </item>
    <item>
      <title>Getting Started</title>
      <link>//localhost:1313/4_modulefour/41_gettingstarted.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/4_modulefour/41_gettingstarted.html</guid>
      <description>Getting Started You can visit the Vector stores page by selecting the Vector stores option located in the LLM Labs sidebar.</description>
    </item>
    <item>
      <title>Getting Started with the Sandbox</title>
      <link>//localhost:1313/5_modulefive_compare/41_gettingstarted.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/5_modulefive_compare/41_gettingstarted.html</guid>
      <description>Getting Started with the Sandbox Step 1: Create Your Sandbox Create Sandbox&#xA;Click on &amp;ldquo;Create Sandbox&amp;rdquo; to establish your dedicated workspace. You can assign a descriptive name to your sandbox for easy identification. Step 2: Configure the Sandbox Application Configure the Application&#xA;Access the &amp;ldquo;Models Configuration&amp;rdquo; section. You can adjust various parameters for the connected LLM model, such as temperature or token settings. Here we will test multiple Amazon Bedrock models to discover the best performing and cost efficient model.</description>
    </item>
    <item>
      <title>Cost Prediction Calculation</title>
      <link>//localhost:1313/5_modulefive_compare/42_costprediction.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/5_modulefive_compare/42_costprediction.html</guid>
      <description>Cost Prediction Calculation Overview The Cost Prediction for Inference feature aims to enhance user experience by providing transparency and predictability in terms of the costs incurred during language model inference. Users can now estimate the financial implications of their inferencing activities within LLM Lab.&#xA;When looking at the responses that were returned from your prompt, you’ll see a cost indication for each return. Cost Prediction Calculation&#xA;The cost prediction will calculate the cost based on your available prompt template, the more prompt template you have the cost will be more expensive.</description>
    </item>
    <item>
      <title>Integrating LLMs Provider</title>
      <link>//localhost:1313/3_modulethree/32_integrating.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/3_modulethree/32_integrating.html</guid>
      <description>Integrating LLMs Provider You can also integrate into several LLMs providers such as Amazon SageMaker JumpStart, OpenAI, and Azure OpenAI by clicking the Manage providers button.&#xA;To integrate the LLMs provider, you will need to set up and add their providers credentials in Datasaur. Once the provider is integrated, you can deploy and use their own models in Datasaur. Deploy the models After integrating the LLMs provider, you can deploy your own model in Datasaur.</description>
    </item>
    <item>
      <title>Vector Store Creation</title>
      <link>//localhost:1313/4_modulefour/42_vectorestorecreation.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/4_modulefour/42_vectorestorecreation.html</guid>
      <description>Vector Store Creation Click the Create new vector store button.&#xA;Configure your vector store settings.&#xA;The configurations are:&#xA;Name: Your preferred vector store name&#xA;Vector store provider: We support two types of vector store provider:&#xA;Datasaur: Utilizing the Datasaur provider means Datasaur will handle the embedding process for you.&#xA;External: Choosing the External provider allows you to handle the embedding process independently.&#xA;Embedding model: Your preferred embedding models. Several embedding models that we support by default are:</description>
    </item>
    <item>
      <title>Deploying the LLM</title>
      <link>//localhost:1313/5_modulefive_compare/43_deployingllm.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/5_modulefive_compare/43_deployingllm.html</guid>
      <description>Deploying the LLM Once you&amp;rsquo;ve crafted the perfect LLM configuration within the Sandbox, you can seamlessly transition it into a real-world application.&#xA;This configured Sandbox environment translates to your LLM Application. LLM Labs empowers you to effortlessly deploy your LLM Application, making it accessible via API for integration into your workflows.&#xA;Deployment Page&#xA;The deployment process is designed for simplicity. Here&amp;rsquo;s how to deploy your LLM Application:&#xA;Navigate to the Deployment Page: Within the Sandbox, locate the dedicated deployment section.</description>
    </item>
    <item>
      <title>Knowledge Base</title>
      <link>//localhost:1313/4_modulefour/43_knowledgebase.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/4_modulefour/43_knowledgebase.html</guid>
      <description>Knowledge Base Once the vector store is created, you can add your files for embedding by uploading them to the Knowledge Base. You can also add your files via the External Object Storage.&#xA;After you select the files, please click on the &amp;ldquo;Update Vector Store&amp;rdquo; button to initiate the embedding process. The embedding process might take some time, depending on the file size and the number of files.&#xA;After completing the embedding process, you can preview the files and use them to conduct Retrieval-Augmented Generation (RAG) in LLM Labs.</description>
    </item>
    <item>
      <title>My Models</title>
      <link>//localhost:1313/3_modulethree/33_mymodels.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/3_modulethree/33_mymodels.html</guid>
      <description>My Models The My Models page allows you to view all deployed models and provides functionality for deploying and undeploying models.&#xA;By default, Datasaur already provides you with Direct Access LLMs that are ready to use on this My models page. Every new model that you deploy in the LLMs providers will be synced to Datasaur, and you can use it right away in Datasaur. If the models you just deployed haven&amp;rsquo;t appeared on My models page, you can click the Sync models button.</description>
    </item>
    <item>
      <title>Disconnect the LLMs provider</title>
      <link>//localhost:1313/3_modulethree/34_disconnect.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/3_modulethree/34_disconnect.html</guid>
      <description>Disconnect the LLMs Provider You can disconnect the LLMs provider by clicking the Manage providers button. The Manage providers dialog will be shown, and you can see the providers that you have already connected.&#xA;Click the See details button, and you will see the Disconnect button in the bottom left of the dialog.&#xA;Once you’ve clicked the Disconnect button, the providers will be disconnected from your workspace.</description>
    </item>
    <item>
      <title>Search</title>
      <link>//localhost:1313/4_modulefour/44_search.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/4_modulefour/44_search.html</guid>
      <description>Search The search function allows you to validate the effectiveness of your knowledge base in providing context. The search results are shown in chunks that follow the chunk size and overlap value you specified when creating the vector store. Each chunk will have a similarity score along with its source. A higher similarity score means the chunk content is more related to the given prompt.</description>
    </item>
    <item>
      <title>Activity</title>
      <link>//localhost:1313/4_modulefour/45_activity.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/4_modulefour/45_activity.html</guid>
      <description>Activity The Activity feature logs all actions performed on your Vector stores, making it easier to track changes and actions. You can filter the activity based on member, file, file source, and date.</description>
    </item>
  </channel>
</rss>
